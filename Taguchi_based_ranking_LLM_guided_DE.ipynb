{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import pathlib\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from accelerate import Accelerator\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, mean_absolute_error\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from pyDOE2 import lhs\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_groq import ChatGroq\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e06f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "POP_SIZE = 10\n",
    "MAX_GEN = 10\n",
    "save_dir = './'\n",
    "# Data Path\n",
    "train_path = 'Training data path'\n",
    "test_path = 'Testing data path'\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7add74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "# Transforms\n",
    "def dataloader():\n",
    "    train_transformer = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10), \n",
    "        transforms.ToTensor(),  \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "    \n",
    "    data = torchvision.datasets.ImageFolder(train_path,transform = train_transformer)\n",
    "    \n",
    "    tr_s = int(train_split * len(data))\n",
    "    val_s = len(data) - tr_s\n",
    "    train, val = random_split(data, [tr_s, val_s])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train,\n",
    "        batch_size = BATCH_SIZE, shuffle = True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val,\n",
    "        batch_size = BATCH_SIZE, shuffle = False\n",
    "    )\n",
    "\n",
    "    test_transformer = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_loader = DataLoader(\n",
    "        torchvision.datasets.ImageFolder(test_path,transform = test_transformer),\n",
    "        batch_size = BATCH_SIZE, shuffle = False\n",
    "    )\n",
    "\n",
    "    root=pathlib.Path(train_path)\n",
    "    classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "    return train_loader, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35109c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data Set\n",
    "accelerator = Accelerator()\n",
    "device = torch.device(\"cuda\")\n",
    "device = accelerator.device\n",
    "train_loader, val_loader, classes = dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = accelerator.prepare(train_loader, val_loader)\n",
    "num_class = len(classes)\n",
    "print(f\"Number of classes: {num_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48960a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latin Hypercube sampeling\n",
    "\n",
    "rate_lhs = lhs(2, 1)[0]\n",
    "rate = {\n",
    "    'F': {\n",
    "        'low': 0.5,\n",
    "        'high': 1,\n",
    "        'type': 'float'\n",
    "    }\n",
    "}\n",
    "rates = {}\n",
    "param_keys = list(rate.keys())\n",
    "for i, param in enumerate(param_keys):\n",
    "        param_info = rate[param]\n",
    "        if param_info[\"type\"] == \"float\":\n",
    "            # Scale [0, 1] sample to the float range\n",
    "            rates[param] = round(param_info[\"low\"] + rate_lhs[i] * (param_info[\"high\"] - param_info[\"low\"]),1)\n",
    "        \n",
    "        \n",
    "SCALING_FAC  = rates['F']         # F\n",
    "print(rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02997f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 DR, \n",
    "                 patch_size: int, \n",
    "                 emb_dim: int, \n",
    "                 in_channels: int = 3, \n",
    "                 img_size: int = 224):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        # self.batch_size = batch_size\n",
    "        assert img_size % self.patch_size == 0, f\"Input img must be divisble by patch size {self.patch_size}\"\n",
    "        self.num_patches = (img_size * img_size) // (self.patch_size ** 2)\n",
    "\n",
    "        self.patcher = nn.Conv2d(\n",
    "                in_channels = self.in_channels,\n",
    "                out_channels = self.emb_dim,\n",
    "                kernel_size = self.patch_size,\n",
    "                stride = self.patch_size,\n",
    "                padding = 0\n",
    "                    )\n",
    "        self.flatten = nn.Flatten(2)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 1, self.emb_dim),\n",
    "            requires_grad = True\n",
    "        )\n",
    "        self.pos_embd = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, self.emb_dim),\n",
    "            requires_grad = True\n",
    "        )\n",
    "        self.emb_dropout = nn.Dropout(p = DR)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.batch_size = x.shape[0]\n",
    "        img_res = x.shape[-1]\n",
    "        assert img_res % self.patch_size == 0, f\"Input img must be divisble by patch size {self.patch_size} and current image shape {img_res}\"\n",
    "        cls_token = self.cls_token.expand(self.batch_size, -1, -1)\n",
    "        x = self.patcher(x)\n",
    "        x = self.flatten(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        # x = x.permute(0, 2 , 1)\n",
    "        x = torch.cat((cls_token, x), dim = 1)\n",
    "        x = x + self.pos_embd\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Step(2) ---> Create ViT Model\n",
    "class MyViT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                DR,\n",
    "                activation,\n",
    "                patch_size,\n",
    "                emb_dim, \n",
    "                num_layers,\n",
    "                num_heads,\n",
    "                num_classes,\n",
    "                d_ff,\n",
    "                in_channels: int = 3,\n",
    "                img_size: int = 224): #224\n",
    "        super().__init__()\n",
    "        self.mlp_size = d_ff #4 * emb_dim\n",
    "        assert img_size % patch_size == 0, \"Img Size must be divisble patch size\"\n",
    "        self.embedding = Embedding(\n",
    "                                  DR = DR,\n",
    "                                  patch_size = patch_size,\n",
    "                                  emb_dim = emb_dim,\n",
    "                                  in_channels = in_channels,\n",
    "                                  img_size = img_size\n",
    "                                  )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "                                encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                            d_model = emb_dim,\n",
    "                                                            nhead = num_heads,\n",
    "                                                            dim_feedforward = self.mlp_size,\n",
    "                                                            activation = activation,\n",
    "                                                            batch_first = True,\n",
    "                                                            norm_first = True), # Create a single Transformer Encoder Layer\n",
    "                                                    num_layers = num_layers\n",
    "                                            )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape = emb_dim, eps = 1e-12),\n",
    "            nn.Linear(in_features = emb_dim,\n",
    "                     out_features = num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.mlp_head(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b808a94",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lr(step, dim_embed, warmup_steps):\n",
    "    return dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 dim_embed,\n",
    "                 warmup_steps,\n",
    "                 steps_in_epoch,\n",
    "                 last_epoch=-1,\n",
    "                 verbose=False):\n",
    "\n",
    "        self.dim_embed = dim_embed\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optimizer.param_groups)\n",
    "\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "        self._step_count = (last_epoch+1)*steps_in_epoch\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = calc_lr(self._step_count, self.dim_embed, self.warmup_steps)\n",
    "        return [lr] * self.num_param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_candidate(candidate, N):\n",
    "    try:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        # LOAD PARAMS\n",
    "        emb_size = hp[\"emb_size\"][\"choices\"][candidate[\"emb_size\"]] \n",
    "        num_heads = hp[\"num_head\"][\"choices\"][candidate[\"num_head\"]]\n",
    "        d_ff = hp[\"d_ff\"][\"choices\"][candidate[\"d_ff\"]]\n",
    "#         num_layers = candidate[\"num_layers\"]\n",
    "#         DR = candidate[\"DR\"]\n",
    "        acf = hp[\"acf\"][\"choices\"][candidate[\"acf\"]]\n",
    "        patch_size = hp[\"patch_size\"][\"choices\"][candidate[\"patch_size\"]]\n",
    "#         lr = candidate[\"lr\"]\n",
    "\n",
    "        # Create Model\n",
    "        print(\".....Creating model.....\")\n",
    "        model = MyViT(\n",
    "                DR = 0.1,\n",
    "                activation = acf,\n",
    "                patch_size = patch_size,\n",
    "                emb_dim = emb_size, \n",
    "                num_layers = 12,\n",
    "                num_heads = num_heads,\n",
    "                num_classes = num_class,\n",
    "                d_ff = d_ff,\n",
    "                in_channels = 3,\n",
    "                img_size = 224   \n",
    "        ).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr = 0.000050, \n",
    "            betas = (0.9, 0.999),\n",
    "            eps = 1.0e-9,\n",
    "            weight_decay = 1e-4\n",
    "        )\n",
    "        model, optimizer, criterion = accelerator.prepare(model, optimizer, criterion)\n",
    "        warmup_steps = len(train_loader)*5\n",
    "\n",
    "        scheduler = Scheduler(\n",
    "            optimizer,\n",
    "            dim_embed = emb_size,\n",
    "            warmup_steps = warmup_steps,\n",
    "            steps_in_epoch = len(train_loader),\n",
    "        )\n",
    "        N_EPOCHS = N\n",
    "        early_stopping_patience = 4\n",
    "        best_val_loss = float(\"inf\")\n",
    "        print(\"------------TRAIN - VAL - LOOP--------------\")\n",
    "        \n",
    "        total_start_time = time.time()  # Start total timer\n",
    "        \n",
    "        # ------------ TRAIN -- LOOP ----------------\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            # torch.cuda.synchronize()\n",
    "            # torch.cuda.empty_cache()\n",
    "            train_loss = 0.0\n",
    "            model.train()\n",
    "            correctt, totalt = 0, 0\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=True):\n",
    "                x, y = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.amp.autocast(device_type = 'cuda', dtype = torch.float16):      \n",
    "                    y_hat = model(x)\n",
    "                    del x\n",
    "                    loss = criterion(y_hat.to(device), y)\n",
    "                scaler.scale(loss).backward() #Change from loss.backward()\n",
    "                # optimizer.step()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                train_loss = train_loss + loss.item() / len(train_loader)\n",
    "                correctt = correctt + (torch.argmax(y_hat, dim=1) == y).sum().item()\n",
    "                totalt = totalt + y.size(0)\n",
    "        # ----------- VALIDATION -------------\n",
    "            with torch.no_grad():\n",
    "                correct, total = 0, 0\n",
    "                test_loss = 0.0\n",
    "                all_preds, all_target = [], []\n",
    "                for batch in tqdm(val_loader, desc=\"Testing\"):\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    y_hat = model(x)\n",
    "                    del x\n",
    "                    y_hat_cpu = y_hat.to(\"cpu\")\n",
    "                    y_cpu = y.to(\"cpu\")\n",
    "                    loss = criterion(y_hat.to(device), y)\n",
    "                    test_loss = test_loss + loss.item() / len(val_loader)\n",
    "        \n",
    "                    correct = correct + (torch.argmax(y_hat, dim=1) == y).sum().item()\n",
    "                    all_preds.append(torch.argmax(y_hat_cpu, dim = 1).flatten().tolist())\n",
    "                    all_target.append(torch.flatten(y_cpu).tolist())\n",
    "                    total = total + y.size(0)\n",
    "            \n",
    "            test_acc = round(correct / total, 4)\n",
    "            print(f\"Epoch: {epoch + 1}/{N_EPOCHS} Train loss: {train_loss:.2f} Train accuracy: {correctt / totalt * 100:.2f}% Val loss: {test_loss:.2f} Val accuracy: {correct / total * 100:.2f}%\")\n",
    "            # Early Stopping\n",
    "            if test_loss < best_val_loss:\n",
    "                best_val_loss = test_loss\n",
    "                no_improvement_counter = 0\n",
    "            else:\n",
    "                no_improvement_counter = no_improvement_counter + 1\n",
    "                if no_improvement_counter >= early_stopping_patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        total_end_time = time.time()  # End total timer\n",
    "        print(f\"Total time taken: {(total_end_time - total_start_time):.2f} seconds\")\n",
    "        \n",
    "        \n",
    "    except Exception as err:\n",
    "        print(\"ERROR while evaluating candidate!\")\n",
    "        print(err)\n",
    "        return float(\"inf\"), 0.0\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1947af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(candidate, N):\n",
    "    try:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        # LOAD PARAMS\n",
    "        emb_size = candidate[\"emb_size\"]\n",
    "        num_heads = candidate[\"num_head\"]\n",
    "        d_ff = candidate[\"d_ff\"]\n",
    "        num_layers = candidate[\"num_layers\"]\n",
    "        DR = candidate[\"DR\"]\n",
    "        acf = candidate[\"acf\"]\n",
    "        patch_size = candidate[\"patch_size\"]\n",
    "        lr = candidate[\"lr\"]\n",
    "\n",
    "        # Create Model\n",
    "        print(\".....Creating model.....\")\n",
    "        model = MyViT(\n",
    "                DR = DR,\n",
    "                activation = acf,\n",
    "                patch_size = patch_size,\n",
    "                emb_dim = emb_size, \n",
    "                num_layers = num_layers,\n",
    "                num_heads = num_heads,\n",
    "                num_classes = num_class,\n",
    "                d_ff = d_ff,\n",
    "                in_channels = 3,\n",
    "                img_size = 224   \n",
    "        ).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr = lr, \n",
    "            betas = (0.9, 0.999),\n",
    "            eps = 1.0e-9,\n",
    "            weight_decay = 1e-4\n",
    "        )\n",
    "        model, optimizer, criterion = accelerator.prepare(model, optimizer, criterion)\n",
    "        warmup_steps = len(train_loader)*5\n",
    "\n",
    "        scheduler = Scheduler(\n",
    "            optimizer,\n",
    "            dim_embed = emb_size,\n",
    "            warmup_steps = warmup_steps,\n",
    "            steps_in_epoch = len(train_loader),\n",
    "        )\n",
    "        N_EPOCHS = N\n",
    "        early_stopping_patience = 4\n",
    "        best_val_loss = float(\"inf\")\n",
    "        print(\"------------TRAIN - VAL - LOOP--------------\")\n",
    "        \n",
    "        total_start_time = time.time()  # Start total timer\n",
    "        \n",
    "        # ------------ TRAIN -- LOOP ----------------\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            train_loss = 0.0\n",
    "            model.train()\n",
    "            correctt, totalt = 0, 0\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=True):\n",
    "                x, y = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.amp.autocast(device_type = 'cuda', dtype = torch.float16):      \n",
    "                    y_hat = model(x)\n",
    "                    del x\n",
    "                    loss = criterion(y_hat.to(device), y)\n",
    "                scaler.scale(loss).backward() \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                train_loss = train_loss + loss.item() / len(train_loader)\n",
    "                correctt = correctt + (torch.argmax(y_hat, dim=1) == y).sum().item()\n",
    "                totalt = totalt + y.size(0)\n",
    "        # ----------- VALIDATION -------------\n",
    "            with torch.no_grad():\n",
    "                correct, total = 0, 0\n",
    "                test_loss = 0.0\n",
    "                all_preds, all_target = [], []\n",
    "                for batch in tqdm(val_loader, desc=\"Testing\"):\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    y_hat = model(x)\n",
    "                    del x\n",
    "                    y_hat_cpu = y_hat.to(\"cpu\")\n",
    "                    y_cpu = y.to(\"cpu\")\n",
    "                    loss = criterion(y_hat.to(device), y)\n",
    "                    test_loss = test_loss + loss.item() / len(val_loader)\n",
    "        \n",
    "                    correct = correct + (torch.argmax(y_hat, dim=1) == y).sum().item()\n",
    "                    all_preds.append(torch.argmax(y_hat_cpu, dim = 1).flatten().tolist())\n",
    "                    all_target.append(torch.flatten(y_cpu).tolist())\n",
    "                    total = total + y.size(0)\n",
    "            \n",
    "            test_acc = round(correct / total, 4)\n",
    "            print(f\"Epoch: {epoch + 1}/{N_EPOCHS} Train loss: {train_loss:.2f} Train accuracy: {correctt / totalt * 100:.2f}% Val loss: {test_loss:.2f} Val accuracy: {correct / total * 100:.2f}%\")\n",
    "            # Early Stopping\n",
    "            if test_loss < best_val_loss:\n",
    "                best_val_loss = test_loss\n",
    "                no_improvement_counter = 0\n",
    "            else:\n",
    "                no_improvement_counter = no_improvement_counter + 1\n",
    "                if no_improvement_counter >= early_stopping_patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        total_end_time = time.time()  # End total timer\n",
    "        print(f\"Total time taken: {(total_end_time - total_start_time):.2f} seconds\")\n",
    "        \n",
    "        \n",
    "    except Exception as err:\n",
    "        print(\"ERROR while evaluating candidate!\")\n",
    "        print(err)\n",
    "        return float(\"inf\"), 0.0\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa5894",
   "metadata": {},
   "source": [
    "# DOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2be25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_candidates_taguchi_L18():\n",
    "    \"\"\"\n",
    "    Generate candidates for hyperparameters using a Taguchi L18 orthogonal array.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of candidate dictionaries.\n",
    "    \"\"\"\n",
    "    # Define the discrete levels for each hyperparameter (ensuring correct indexing) On the basis of servay\n",
    "    levels = {\n",
    "        \"acf\": [\"relu\", \"gelu\"],                     \n",
    "        \"patch_size\": [4, 8, 16],                     \n",
    "        \"emb_size\": [32, 128, 512],     \n",
    "        \"num_layers\": [4, 12, 16],             \n",
    "        \"num_head\": [4, 8, 16],                    \n",
    "        \"DR\": [0.1, 0.3, 0.5],             \n",
    "        \"d_ff\": [48, 192, 768],   \n",
    "        \"lr\": [1e-6, 5e-5, 1e-4]              \n",
    "    }\n",
    "\n",
    "    # Define the corrected Taguchi L18 orthogonal array (valid values)\n",
    "    taguchi_array = np.array([\n",
    "        [1,1,1,1,1,1,1,1], [1,1,2,2,2,2,2,2],\n",
    "        [1,1,3,3,3,3,3,3], [1,2,1,2,3,3,2,1],\n",
    "        [1,2,2,3,1,1,3,2], [1,2,3,1,2,2,1,3],\n",
    "        [1,3,1,3,2,1,2,2], [1,3,2,1,3,2,3,3],\n",
    "        [1,3,3,2,1,3,1,1], [2,1,1,2,3,2,3,1],\n",
    "        [2,1,2,3,1,3,1,2], [2,1,3,1,2,1,2,3],\n",
    "        [2,2,1,3,1,1,2,3], [2,2,2,1,2,2,3,1],\n",
    "        [2,2,3,2,3,3,1,2], [2,3,1,1,3,3,1,2],\n",
    "        [2,3,2,2,1,1,2,3], [2,3,3,3,2,2,3,1]\n",
    "    ])\n",
    "\n",
    "    candidates = []\n",
    "    for row in taguchi_array:\n",
    "        candidate = {\n",
    "            \"acf\": levels[\"acf\"][row[0] - 1],\n",
    "            \"patch_size\": levels[\"patch_size\"][row[1] - 1],\n",
    "            \"emb_size\": levels[\"emb_size\"][row[2] - 1],\n",
    "            \"num_layers\": levels[\"num_layers\"][row[3] - 1],\n",
    "            \"num_head\": levels[\"num_head\"][row[4] - 1],\n",
    "            \"DR\": levels[\"DR\"][row[5] - 1],\n",
    "            \"d_ff\": levels[\"d_ff\"][row[6] - 1],\n",
    "            \"lr\": levels[\"lr\"][row[7] - 1]\n",
    "        }\n",
    "        candidates.append(candidate)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "# Example usage:\n",
    "candidates_L18 = generate_candidates_taguchi_L18()\n",
    "print(\"Generated Candidates using Taguchi L18:\")\n",
    "for i, cand in enumerate(candidates_L18):\n",
    "    print(f\"Candidate {i+1}: {cand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff8d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taguchi_acc = []\n",
    "taguchi_loss = []\n",
    "for i, cand in enumerate(candidates_L18):\n",
    "    print(f\"Candidate {i+1}: {cand}\")  \n",
    "    loss, acc = experiment(cand, 50) \n",
    "    taguchi_acc.append(acc)\n",
    "    taguchi_loss.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49562a97",
   "metadata": {},
   "source": [
    "# Finding the ranking of Hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dae560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(candidates_L18)\n",
    "df[\"accuracy\"] = taguchi_acc\n",
    "df[\"loss\"] = taguchi_loss\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statistical Analysis\n",
    "## Mean and Standard Deviation\n",
    "mean_acc = df[\"accuracy\"].mean()\n",
    "mean_loss = df[\"loss\"].mean()\n",
    "std_acc = df[\"accuracy\"].std()\n",
    "std_loss = df[\"loss\"].std()\n",
    "\n",
    "print(f\"Mean Of Accuracy And Loss: {mean_acc:.2f} and {mean_loss:.2f}\")\n",
    "print(f\"Standard Deviation Of Accuracy And Loss: {std_acc:.2f} and {std_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d78140",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perfom ANOVA \n",
    "df = pd.get_dummies(df, columns = [\"acf\"], drop_first = True)\n",
    "anova_acc = f_oneway(df[\"accuracy\"], \n",
    "                     df[\"lr\"],\n",
    "                     df[\"patch_size\"],\n",
    "                     df[\"emb_size\"],\n",
    "                     df[\"num_layers\"],\n",
    "                     df[\"num_head\"],\n",
    "                     df[\"DR\"], \n",
    "                     df[\"acf_relu\"],\n",
    "                     df[\"d_ff\"])\n",
    "\n",
    "anova_loss = f_oneway(df[\"loss\"], \n",
    "                     df[\"lr\"],\n",
    "                     df[\"patch_size\"],\n",
    "                     df[\"emb_size\"],\n",
    "                     df[\"num_layers\"],\n",
    "                     df[\"num_head\"],\n",
    "                     df[\"DR\"], \n",
    "                     df[\"acf_relu\"],\n",
    "                     df[\"d_ff\"])\n",
    "\n",
    "print(f\"ANOVA For Accuracy test p-value = {anova_acc.pvalue:.12f} and test statistic = {anova_acc.statistic:.5f}\")\n",
    "print(f\"ANOVA For Loss test p-value = {anova_loss.pvalue:.12f} and test statistic = {anova_loss.statistic:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Importance Analysis\n",
    "X = df.iloc[:, [0,1,2,3,4,5,6,9]]\n",
    "y = df[[\"accuracy\", \"loss\"]]\n",
    "rf = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "rf.fit(X, y)\n",
    "feature_imp = pd.Series(rf.feature_importances_, index = X.columns)\n",
    "print(\"\\nHP Importance:\\n\")\n",
    "print(feature_imp.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sns.barplot(x = feature_imp.values,\n",
    "            y = feature_imp.index,\n",
    "            palette = \"viridis\")\n",
    "plt.title(\"HP Importance In Accuracy And Loss Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "for i, col in enumerate(df.columns[[0,1,2,3,4,5,6,9]]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    sns.boxplot(x = df[col], y = df[\"accuracy\"])\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(f\"Impact of {col} on accuracy\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "for i, col in enumerate(df.columns[[0,1,2,3,4,5,6,9]]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    sns.boxplot(x = df[col], y = df[\"loss\"])\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(f\"Impact of {col} on loss\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc573fd8",
   "metadata": {},
   "source": [
    "# Redefine Hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b540a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine Hyper parameters \n",
    "hp = {\n",
    "    'Based on importance ranking redefine the hp'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88965b66",
   "metadata": {},
   "source": [
    "# LLM+DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(x, low, high):\n",
    "    return max(low, min(x, high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a53f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Generate candidate \n",
    "def generate_candidate():\n",
    "    candidate = {}\n",
    "    for param in hp.keys():\n",
    "        if hp[param][\"type\"] == \"float\":\n",
    "            candidate[param] = random.uniform(hp[param][\"low\"], hp[param][\"high\"])\n",
    "        elif hp[param][\"type\"] == \"int\" or hp[param][\"type\"] == \"choice\":\n",
    "            candidate[param] = random.randint(hp[param][\"low\"], hp[param][\"high\"])\n",
    "    return candidate\n",
    "\n",
    "# Using DOE generate candidate\n",
    "def generate_candidate_doe(hp):\n",
    "    param_keys = list(hp.keys())\n",
    "    n_params = len(param_keys)\n",
    "    # Generate LHS samples in [0, 1]\n",
    "    lhs_sample = lhs(n_params, samples=1)[0]\n",
    "    # Scale samples to the hyperparameter ranges\n",
    "    candidate = {}\n",
    "    for i, param in enumerate(param_keys):\n",
    "            param_info = hp[param]\n",
    "            if param_info[\"type\"] == \"float\":\n",
    "                # Scale [0, 1] sample to the float range\n",
    "                candidate[param] = param_info[\"low\"] + lhs_sample[i] * (param_info[\"high\"] - param_info[\"low\"])\n",
    "            elif param_info[\"type\"] == \"int\":\n",
    "                # Scale and round to integer range\n",
    "                candidate[param] = int(round(param_info[\"low\"] + lhs_sample[i] * (param_info[\"high\"] - param_info[\"low\"])))\n",
    "            elif param_info[\"type\"] == \"choice\":\n",
    "                # Treat as discrete choices\n",
    "                candidate[param] = int(round(param_info[\"low\"] + lhs_sample[i] * (param_info[\"high\"] - param_info[\"low\"])))\n",
    "    # for i, param in enumerate(param_keys):\n",
    "    #     param_info = hp[param]\n",
    "    #     if param_info[\"type\"] == \"float\":\n",
    "    #         # Scale [0, 1] sample to the float range\n",
    "    #         candidate[param] = param_info[\"low\"] + lhs_sample[i] * (param_info[\"high\"] - param_info[\"low\"])\n",
    "    #     elif param_info[\"type\"] == \"int\":\n",
    "    #         # Scale and round to integer range\n",
    "    #         candidate[param] = int(round(param_info[\"low\"] + lhs_sample[i] * (param_info[\"high\"] - param_info[\"low\"])))\n",
    "    #     elif param_info[\"type\"] == \"choice\":\n",
    "    #         # Map sample to discrete choices\n",
    "    #         idx = int(lhs_sample[i] * len(param_info[\"choices\"]))  # Scale to choice indices\n",
    "    #         candidate[param] = param_info[\"choices\"][min(idx, len(param_info[\"choices\"]) - 1)]\n",
    "    return candidate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Candidate\n",
    "def print_candidate(candidate):\n",
    "    readable_candidate = {}\n",
    "    for param in candidate.keys():\n",
    "        if hp[param][\"type\"] == \"choice\":\n",
    "            choice = candidate[param]\n",
    "            readable_candidate[param] = hp[param][\"choices\"][choice]\n",
    "        else:\n",
    "            readable_candidate[param] = candidate[param]\n",
    "    print(readable_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcc207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Population\n",
    "def generate_population(POP_SIZE = 5):\n",
    "    return [{\"candidate\": generate_candidate_doe(hp), \"score\": float(\"inf\"), \"Acc\": 0} for _ in range(POP_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c800ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = generate_population(POP_SIZE = POP_SIZE)\n",
    "print(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_population(population):\n",
    "    for p in population:\n",
    "        print(f\"{p['score']:.4f}\", end=\" -> \")\n",
    "        print(f\"{p['Acc']:.4f}\", end = \" -> \")\n",
    "        print_candidate(p['candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE SCORES FOR INITIAL POPULATION\n",
    "def cal_score_intial_pop(population, N):\n",
    "    for i in range(len(population)):\n",
    "        print(f\"-------------------------[ CANDIDATE {i+1:2d} ]-------------------------\")\n",
    "        candidate = population[i][\"candidate\"]\n",
    "        print_candidate(candidate)\n",
    "        score, Acc = evaluate_candidate(candidate , N)\n",
    "        population[i][\"score\"] = score\n",
    "        population[i][\"Acc\"] = Acc\n",
    "    best_index = np.argmin([c[\"score\"] for c in population])\n",
    "    best_score = population[best_index][\"score\"]\n",
    "    best_Acc = population[best_index][\"Acc\"]\n",
    "    best_candidate = population[best_index][\"candidate\"]\n",
    "    return best_candidate, best_score, best_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY =  \"Your API Key\"\n",
    "def prompt(target_vector, donor_vector):\n",
    "    \"\"\"\n",
    "    Generate a natural language prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\" I am running a hyper parameter tuning of ViT model using differential algorithm. I give you my trail vector and target vector after mutation.Your role is to perform the cross over as you are the expert on cross over operation.\\n\"\n",
    "        f\" - Here's the donor vector: {donor_vector}\\n\"\n",
    "        f\" - Here's the traget vector: {target_vector}\\n\"\n",
    "        f\" - Now perform the cross over. Choose the float values from the target and donor vector on the basis of normal distribution.\\n\"\n",
    "        f\" Return the trail vector in jeson format (NO PREAMBLE).\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a867e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_guided_crossover(target_vector, donor_vector):\n",
    "    llm = ChatGroq(\n",
    "    temperature = 0,\n",
    "    groq_api_key = API_KEY,\n",
    "    model_name = 'llama-3.3-70b-versatile'   \n",
    "    )\n",
    "    chat = prompt(target_vector, donor_vector)\n",
    "    response = llm.invoke(chat)\n",
    "    json_parser = JsonOutputParser()\n",
    "    json_res = json_parser.parse(response.content)\n",
    "    return json_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3306f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def De(population = population,\n",
    "       MAX_GEN = MAX_GEN, \n",
    "       F = SCALING_FAC, \n",
    "    #    CR = CROSSOVER_RATE, \n",
    "       N = 50):\n",
    "    print_population(population)\n",
    "    best_candidate, best_score, best_Acc = cal_score_intial_pop(population, N)\n",
    "    for G in range(MAX_GEN):\n",
    "        print(f\"=========================[ GENERATION {G+1:2d} ]=========================\")\n",
    "        for i in range(len(population)):\n",
    "            print(f\"-------------------------[ CANDIDATE  {i+1:2d} ]-------------------------\")\n",
    "            target_vector = population[i][\"candidate\"]\n",
    "            print(\"target_vector:\", end=\" \")\n",
    "            print_candidate(target_vector)\n",
    "            # GENERATE\n",
    "            choices = list(range(0, i)) + list(range(i+1, POP_SIZE))  # make chance of picking ith candidate 0\n",
    "            a, b, c = np.random.choice(choices, 3, replace=False)\n",
    "            x1 = population[a][\"candidate\"]\n",
    "            x2 = population[b][\"candidate\"]\n",
    "            x3 = population[c][\"candidate\"]\n",
    "            print(f\"X_r1 = {x1}\")\n",
    "            print(f\"X_r2 = {x2}\")\n",
    "            print(f\"X_r3 = {x3}\")\n",
    "            # MUTATION\n",
    "            donor_vector = {}\n",
    "            for param in target_vector.keys():\n",
    "                donor_vector[param] = x1[param] + F * (x2[param] - x3[param])\n",
    "                if hp[param][\"type\"] in (\"int\", \"choice\"):\n",
    "                    donor_vector[param] = round(donor_vector[param])\n",
    "                donor_vector[param] = clamp(donor_vector[param], hp[param][\"low\"], hp[param][\"high\"])\n",
    "            print(\"donor_vector:\", end=\" \")\n",
    "            print_candidate(donor_vector)\n",
    "\n",
    "            # LLM BASED CROSSOVER\n",
    "            try:\n",
    "                trial_vector = llm_guided_crossover(target_vector, donor_vector)\n",
    "                print(\"trial_vector:\", end=\" \")\n",
    "                print_candidate(trial_vector)\n",
    "            except Exception as e:\n",
    "                print(\"Error during LLM-guided crossover:\", e) \n",
    "\n",
    "\n",
    "            # EVALUATE\n",
    "            trial_score, trial_acc = evaluate_candidate(trial_vector, N)\n",
    "            if trial_score < population[i][\"score\"]:\n",
    "                print(f\"{trial_score:0.5f} < {population[i]['score']:0.5f}, picking trial_vector\")\n",
    "                population[i][\"candidate\"] = trial_vector\n",
    "                population[i][\"score\"] = trial_score\n",
    "                population[i][\"Acc\"] = trial_acc\n",
    "            else:\n",
    "                print(f\"{trial_score:0.5f} >= {population[i]['score']:0.5f}, keeping target_vector\")\n",
    "\n",
    "                # FIND BEST TILL NOW\n",
    "        best_index = np.argmin([c[\"score\"] for c in population])\n",
    "        new_best_score = population[best_index][\"score\"]\n",
    "        new_best_acc = population[best_index][\"Acc\"]\n",
    "        if new_best_score < best_score:\n",
    "            print(f\"Best score improved from {best_score:0.4f} to {new_best_score:0.4f}\")\n",
    "            best_score = new_best_score\n",
    "            best_Acc = new_best_acc\n",
    "            best_candidate = population[best_index][\"candidate\"]\n",
    "            print(\"Best candidate: \", end=\"\")\n",
    "            print_candidate(best_candidate)\n",
    "\n",
    "        print(\"====================================\")\n",
    "        print(\"Final Best candidate: \", end=\"\")\n",
    "        print_candidate(best_candidate)\n",
    "\n",
    "        # WRITE TO CSV LOG\n",
    "        logfilepath = os.path.join(save_dir,f\"logs_p{POP_SIZE}_bs{BATCH_SIZE}.csv\")\n",
    "        if not os.path.isfile(logfilepath):\n",
    "            with open(logfilepath, \"a\") as logfile:\n",
    "                logfile.write(\"gen,\" + \",\".join(map(str, range(len(population)))) + \"\\n\")\n",
    "\n",
    "        with open(logfilepath, \"a\") as logfile:\n",
    "            logfile.write(f\"{G+1},\" + \",\".join(map(str, [c[\"score\"] for c in population])) + \"\\n\")\n",
    "\n",
    "        print(\"\\nPopulation at end of generation\", G+1)\n",
    "        print_population(population)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "De()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817feb6",
   "metadata": {},
   "source": [
    "# **RUNNING BEST MODEL ARCHITECTURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad96c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
